include "telemetry.conf"

akka {
  loglevel = DEBUG

  actor {
    provider = cluster
    allow-java-serialization = off

    serialization-bindings {
      "cnapp.poc.domain.CborSerializable" = jackson-cbor
    }
  }

  # For the sample, just bind to loopback and do not allow access from the network
  # the port is overridden by the logic in main class
  remote.artery {
    canonical.port = 0
    canonical.hostname = 127.0.0.1
  }

  cluster {
    seed-nodes = [
      "akka://Shopping@127.0.0.1:2551",
      "akka://Shopping@127.0.0.1:2552"
    ]

    roles = ["write-model", "read-model"]

    downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"
  }

  # use Cassandra to store both snapshots and the events of the persistent actors
  persistence {
    journal.plugin = "akka.persistence.cassandra.journal"
    snapshot-store.plugin = "akka.persistence.cassandra.snapshot"
  }

}
# Configuration for akka-persistence-cassandra
akka.persistence.cassandra {
  events-by-tag {
    bucket-size = "Day"
    # for reduced latency
    eventual-consistency-delay = 200ms
    flush-interval = 50ms
    pubsub-notification = on
    first-time-bucket = "20200115T00:00"
  }

  query {
    refresh-interval = 2s
  }

  # don't use autocreate in production
  journal.keyspace-autocreate = on
  journal.tables-autocreate = on
  snapshot.keyspace-autocreate = on
  snapshot.tables-autocreate = on
}
//akka.persistence.journal.plugin = "akka-contrib-mongodb-persistence-journal"
//akka.persistence.snapshot-store.plugin = "akka-contrib-mongodb-persistence-snapshot"
//akka.contrib.persistence.mongodb.mongo {
//  # Caches of collections created by the plugin
//  collection-cache {
//
//    # Cache of journal collections
//    journal {
//      # Implementation of the cache.
//      # - Must be a subtype of MongoCollectionCache.
//      # - Must have a public constructor taking a Config object as argument.
//      # - Must be able to store the collection type of the chosen driver.
//      #
//      # If left empty, a default naive implementation with unbound memory consumption is used.
//      class = ""
//
//      # How long to retain the collection. Invalid or missing durations are treated as eternity.
//      expire-after-write = Infinity
//    }
//
//    # Cache of snapshot collections
//    snaps {
//      class = ""
//      expire-after-write = Infinity
//    }
//
//    # Cache of one realtime collection
//    realtime {
//      class = ""
//      expire-after-write = Infinity
//
//      # maximum size of the cache
//      # 1 because the realtime collection is unique
//      # default caches do not honor size bounds bigger than 1
//      max-size = 1
//    }
//
//    # Cache of one metadata collection
//    metadata {
//      class = ""
//      expire-after-write = Infinity
//
//      # maximum size of the cache
//      # 1 because the metadata collection is unique
//      # default caches do not honor size bounds bigger than 1
//      max-size = 1
//    }
//  }
//}

datastax-java-driver {
  advanced.reconnect-on-init = on
  advanced{
    auth-provider {
    }
  }
}

akka.projection.cassandra.offset-store.keyspace = "akka_cqrs_sample"

event-processor {
  tag-prefix = "carts-slice"       // even processor tag prefix
  parallelism = 4                  // number of event processors
}
shopping.mongo.host = "localhost"
shopping.mongo.port = 27017
shopping.http.port = 0
shopping.askTimeout = 5 s
shopping.kafka.producer {
  bootstrap-servers = "localhost:9092"
  topic = "user-events"
}


